<!DOCTYPE html>
<html lang="de">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="robots" content="noodp" />
        <meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1">
        <title>Decentralized ML: Experiments on Topology and Privacy - Nina Mainusch</title><meta name="Description" content="Dezentralisiertes maschinelles Lernen ist eine wichtige Art des maschinellen Lernens, die verwendet wird, um Probleme wie Datenlokalität, Eigentum, Privatsphäre und hohe Kommunikationskosten zu lösen. Wir leisten einen Beitrag zur empirischen Bewertung der Auswirkungen von Topologie und differentieller Privatsphäre auf Modellkonvergenz und Fehler in einem dezentralen Lernsystem."><meta property="og:title" content="Decentralized ML: Experiments on Topology and Privacy" />
<meta property="og:description" content="Dezentralisiertes maschinelles Lernen ist eine wichtige Art des maschinellen Lernens, die verwendet wird, um Probleme wie Datenlokalität, Eigentum, Privatsphäre und hohe Kommunikationskosten zu lösen. Wir leisten einen Beitrag zur empirischen Bewertung der Auswirkungen von Topologie und differentieller Privatsphäre auf Modellkonvergenz und Fehler in einem dezentralen Lernsystem." />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://example.org/de/posts/decentralized_opt_proj/" /><meta property="og:image" content="http://example.org/logo.png"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-08-28T12:57:25+02:00" />
<meta property="article:modified_time" content="2021-08-28T12:57:25+02:00" />

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="http://example.org/logo.png"/>

<meta name="twitter:title" content="Decentralized ML: Experiments on Topology and Privacy"/>
<meta name="twitter:description" content="Dezentralisiertes maschinelles Lernen ist eine wichtige Art des maschinellen Lernens, die verwendet wird, um Probleme wie Datenlokalität, Eigentum, Privatsphäre und hohe Kommunikationskosten zu lösen. Wir leisten einen Beitrag zur empirischen Bewertung der Auswirkungen von Topologie und differentieller Privatsphäre auf Modellkonvergenz und Fehler in einem dezentralen Lernsystem."/>
<meta name="application-name" content="Nina Mogan">
<meta name="apple-mobile-web-app-title" content="Nina Mogan"><meta name="theme-color" content="#ffffff"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
        <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="http://example.org/de/posts/decentralized_opt_proj/" /><link rel="stylesheet" href="/lib/normalize/normalize.min.css"><link rel="stylesheet" href="/css/style.min.css"><link rel="stylesheet" href="/lib/fontawesome-free/all.min.css"><link rel="stylesheet" href="/lib/animate/animate.min.css"><script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "headline": "Decentralized ML: Experiments on Topology and Privacy",
        "inLanguage": "de",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "http:\/\/example.org\/de\/posts\/decentralized_opt_proj\/"
        },"genre": "posts","keywords": "decentralized, machinelearning, privacy","wordcount":  1950 ,
        "url": "http:\/\/example.org\/de\/posts\/decentralized_opt_proj\/","datePublished": "2021-08-28T12:57:25+02:00","dateModified": "2021-08-28T12:57:25+02:00","license": "This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.","publisher": {
            "@type": "Organization",
            "name": "Nina"},"author": {
                "@type": "Person",
                "name": "Nina"
            },"description": "Dezentralisiertes maschinelles Lernen ist eine wichtige Art des maschinellen Lernens, die verwendet wird, um Probleme wie Datenlokalität, Eigentum, Privatsphäre und hohe Kommunikationskosten zu lösen. Wir leisten einen Beitrag zur empirischen Bewertung der Auswirkungen von Topologie und differentieller Privatsphäre auf Modellkonvergenz und Fehler in einem dezentralen Lernsystem."
    }
    </script></head>
    <body header-desktop="fixed" header-mobile="auto"><script type="text/javascript">(window.localStorage && localStorage.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('theme', 'dark');</script>

        <div id="mask"></div><div class="wrapper"><header class="desktop" id="header-desktop">
    <div class="header-wrapper">
        <div class="header-title">
            <a href="/de/" title="Nina Mainusch">Nina Mogan Mainusch</a>
        </div>
        <div class="menu">
            <div class="menu-inner"><a class="menu-item" href="/de/about/"> Über mich </a><a class="menu-item" href="/de/posts/"> Artikel </a><a class="menu-item" href="https://github.com/nina-mainusch" title="GitHub" rel="noopener noreffer" target="_blank"><i class='fab fa-github fa-fw'></i>  </a><a class="menu-item" href="/de/documentation/test/" title="Rechtsauskünfte"><i class='fa fa-gavel fa-fw'></i>  </a><span class="menu-item delimiter"></span><a href="javascript:void(0);" class="menu-item language" title="Sprache wählen">Deutsch<i class="fas fa-chevron-right fa-fw"></i>
                        <select class="language-select" id="language-select-desktop" onchange="location = this.value;"><option value="/posts/decentralized_opt_proj/">English</option><option value="/de/posts/decentralized_opt_proj/" selected>Deutsch</option><option value="/fr/posts/decentralized_opt_proj/">Français</option></select>
                    </a><span class="menu-item search" id="search-desktop">
                        <input type="text" placeholder="Suche nach Titel und Inhalt..." id="search-input-desktop">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-desktop" title="Suche">
                            <i class="fas fa-search fa-fw"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-desktop" title="Leeren">
                            <i class="fas fa-times-circle fa-fw"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-desktop">
                            <i class="fas fa-spinner fa-fw fa-spin"></i>
                        </span>
                    </span><a href="javascript:void(0);" class="menu-item theme-switch" title="Thema wechseln">
                    <i class="fas fa-adjust fa-fw"></i>
                </a>
            </div>
        </div>
    </div>
</header><header class="mobile" id="header-mobile">
    <div class="header-container">
        <div class="header-wrapper">
            <div class="header-title">
                <a href="/de/" title="Nina Mainusch">Nina Mogan Mainusch</a>
            </div>
            <div class="menu-toggle" id="menu-toggle-mobile">
                <span></span><span></span><span></span>
            </div>
        </div>
        <div class="menu" id="menu-mobile"><div class="search-wrapper">
                    <div class="search mobile" id="search-mobile">
                        <input type="text" placeholder="Suche nach Titel und Inhalt..." id="search-input-mobile">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-mobile" title="Suche">
                            <i class="fas fa-search fa-fw"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-mobile" title="Leeren">
                            <i class="fas fa-times-circle fa-fw"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-mobile">
                            <i class="fas fa-spinner fa-fw fa-spin"></i>
                        </span>
                    </div>
                    <a href="javascript:void(0);" class="search-cancel" id="search-cancel-mobile">
                        Abbrechen
                    </a>
                </div><a class="menu-item" href="/de/about/" title="">Über mich</a><a class="menu-item" href="/de/posts/" title="">Artikel</a><a class="menu-item" href="https://github.com/nina-mainusch" title="GitHub" rel="noopener noreffer" target="_blank"><i class='fab fa-github fa-fw'></i></a><a class="menu-item" href="/de/documentation/test/" title="Rechtsauskünfte"><i class='fa fa-gavel fa-fw'></i></a><a href="javascript:void(0);" class="menu-item theme-switch" title="Thema wechseln">
                <i class="fas fa-adjust fa-fw"></i>
            </a><a href="javascript:void(0);" class="menu-item" title="Sprache wählen">Deutsch<i class="fas fa-chevron-right fa-fw"></i>
                    <select class="language-select" onchange="location = this.value;"><option value="/posts/decentralized_opt_proj/">English</option><option value="/de/posts/decentralized_opt_proj/" selected>Deutsch</option><option value="/fr/posts/decentralized_opt_proj/">Français</option></select>
                </a></div>
    </div>
</header>
<div class="search-dropdown desktop">
    <div id="search-dropdown-desktop"></div>
</div>
<div class="search-dropdown mobile">
    <div id="search-dropdown-mobile"></div>
</div>
<main class="main">
                <div class="container"><div class="toc" id="toc-auto">
            <h2 class="toc-title">Inhalt</h2>
            <div class="toc-content" id="toc-content-auto"></div>
        </div><article class="page single"><h1 class="single-title animated flipInX">Decentralized ML: Experiments on Topology and Privacy</h1><div class="post-meta">
            <div class="post-meta-line"><span class="post-author"><a href="/de/" title="Author" rel=" author" class="author"><i class="fas fa-user-circle fa-fw"></i>Nina</a></span>&nbsp;<span class="post-category">enthalten in <a href="/de/categories/decentralized-ml/"><i class="far fa-folder fa-fw"></i>Decentralized ML</a></span></div>
            <div class="post-meta-line"><i class="far fa-calendar-alt fa-fw"></i>&nbsp;<time datetime="2021-08-28">2021-08-28</time>&nbsp;<i class="fas fa-pencil-alt fa-fw"></i>&nbsp;1950 wörter&nbsp;
                <i class="far fa-clock fa-fw"></i>&nbsp;10 minuten&nbsp;</div>
        </div><div class="details toc" id="toc-static"  kept="">
                <div class="details-summary toc-title">
                    <span>Inhalt</span>
                    <span><i class="details-icon fas fa-angle-right"></i></span>
                </div>
                <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li><a href="#zusammenfassung">Zusammenfassung</a></li>
    <li><a href="#einleitung">Einleitung</a>
      <ul>
        <li><a href="#was-ist-unsere-motivation">Was ist unsere Motivation?</a></li>
        <li><a href="#worum-geht-es-beim-dezentralen-maschinellen-lernen">Worum geht es beim dezentralen maschinellen Lernen?</a></li>
        <li><a href="#topologie">Topologie</a></li>
        <li><a href="#differentielle-privatsphäre">Differentielle Privatsphäre</a></li>
        <li><a href="#verwandte-arbeiten">Verwandte Arbeiten</a></li>
      </ul>
    </li>
    <li><a href="#experimente">Experimente</a>
      <ul>
        <li><a href="#versuchsaufbau">Versuchsaufbau</a></li>
      </ul>
    </li>
    <li><a href="#ergebnisse">Ergebnisse</a></li>
    <li><a href="#schlussfolgerung">Schlussfolgerung</a></li>
  </ul>
</nav></div>
            </div><div class="content" id="content"><h2 id="zusammenfassung">Zusammenfassung</h2>
<p>Dezentrales maschinelles Lernen ist eine wichtige Art des maschinellen Lernens, um Probleme wie Datenlokalität, Eigentum, Privatsphäre und hohe Kommunikationskosten zu lösen.
In unserem Projekt für den Kurs [CS-439 - Optimization for Machine Learning] (<a href="https://edu.epfl.ch/coursebook/en/optimization-for-machine-learning-CS-439" target="_blank" rel="noopener noreffer">https://edu.epfl.ch/coursebook/en/optimization-for-machine-learning-CS-439</a>) an der EPFL haben zwei Mitstudierende und ich eine empirische Bewertung des Einflusses von Topologie und differentieller Privatsphäre auf Modellkonvergenz und Fehler in einem dezentralen Lernsystem durchgeführt. Schau dir gerne unseren <a href="https://github.com/Devrim-Celik/OptimML" target="_blank" rel="noopener noreffer">Github Ordner</a> oder unser <a href="/documents_/OptML_Project_Report.pdf" rel="">Forschungspapier</a> an.</p>
<div class="details admonition tip open">
        <div class="details-summary admonition-title">
            <i class="icon fas fa-lightbulb fa-fw"></i>Wichtigste Erkenntnis<i class="details-icon fas fa-angle-right fa-fw"></i>
        </div>
        <div class="details-content">
            <div class="admonition-content">Die wichtigste Erkenntnis war, dass spärliche Graphen, gemessen an der spektralen Lücke der Topologie, selbst in schwierigen Situationen unerwartet gut abschneiden, während dichter verbundene Graphen mit zunehmender Knotenzahl langsamer lernen.</div>
        </div>
    </div>
<h2 id="einleitung">Einleitung</h2>
<h3 id="was-ist-unsere-motivation">Was ist unsere Motivation?</h3>
<p>Zentralisierte Formen des maschinellen Lernens sind gründlich erforscht worden und stellen den aktuellen Status quo dar, haben aber erhebliche Probleme mit Fragen wie Datenlokalisierung, Eigentum, Datenschutz und hohen Kommunikationskosten.
Dezentrales maschinelles Lernen löst diese Probleme, indem es die Kontrolle von einem primären Server auf jedes einzelne Edge-Gerät überträgt.
Dadurch können die Daten an der Quelle verbleiben, wodurch die Privatsphäre geschützt und die Kommunikationskosten gesenkt werden, indem das Medium von den Daten selbst auf eine komprimierte Darstellung verlagert wird.
Auch wenn dezentrale Netzwerke den Datenschutz verbessern, gibt es immer noch Probleme, da sensible Daten aus etwas so Einfachem wie einem gemeinsamen Gradienten gelernt werden können.
Dies motiviert uns zu der Überlegung, die Kommunikation mit einem differenzierten Datenschutz zu versehen, um ein Maximum an Privatsphäre zu gewährleisten.</p>
<h3 id="worum-geht-es-beim-dezentralen-maschinellen-lernen">Worum geht es beim dezentralen maschinellen Lernen?</h3>
<p><figure><a class="lightgallery" href="/images/centralized-1.png" title="Visuell erklärt" data-thumbnail="/images/centralized-1.png" data-sub-html="<h2>Drei Arten von Organisationsstrukturen werden grafisch beschrieben, wobei Geräteeinheiten durch Rechtecke, zentrale Einheiten durch Wolken und Pfeile für die Kommunikation zwischen den Einheiten dargestellt werden. Jedes Gehirn steht für ein maschinelles Lernmodell, das von der entsprechenden Einheit trainiert wird. Jedes kleine farbige Kästchen steht für Daten, die sich auf einem Gerät befinden, und die farbigen Rechtecke stellen eine transformierte Darstellung der Daten dar.</h2><p>Visuell erklärt</p>">
        <img
            class="lazyload"
            src="/svg/loading.min.svg"
            data-src="/images/centralized-1.png"
            data-srcset="/images/centralized-1.png, /images/centralized-1.png 1.5x, /images/centralized-1.png 2x"
            data-sizes="auto"
            alt="/images/centralized-1.png" />
    </a><figcaption class="image-caption">Drei Arten von Organisationsstrukturen werden grafisch beschrieben, wobei Geräteeinheiten durch Rechtecke, zentrale Einheiten durch Wolken und Pfeile für die Kommunikation zwischen den Einheiten dargestellt werden. Jedes Gehirn steht für ein maschinelles Lernmodell, das von der entsprechenden Einheit trainiert wird. Jedes kleine farbige Kästchen steht für Daten, die sich auf einem Gerät befinden, und die farbigen Rechtecke stellen eine transformierte Darstellung der Daten dar.</figcaption>
    </figure></p>
<p>In einem traditionellen <strong>zentralisierten maschinellen Lernsystem</strong>, wie in der obigen Abbildung, Teil 1, dargestellt, werden die Daten von fremden Geräten zu einem zentralen System übertragen, wo Modelle klassisch mit stochastischem Gradientenabstieg (SGD) trainiert werden können, der für $T$ Iterationen eine Laufzeit von $O(1/\sqrt{T})$ hat, wenn die Zielfunktion konvex ist.</p>
<p>Beim <strong>föderierten learning</strong> (siehe Abbildung, Teil 2) trainiert jedes Gerät gemeinsam ein globales maschinelles Lernmodell und wird von einem zentralen Server organisiert.
Die Daten werden nicht gemeinsam genutzt, und die Geräte senden Aktualisierungen an einen Server, der diese zusammenfasst und wiederum die gemeinsamen Modelle aller Geräte aktualisiert.
Der bekannteste Optimierungsalgorithmus ist die föderierte Mittelwertbildung mit Konvergenz $O(\frac{HM}{T} + \frac{\sigma}{\sqrt{TKM}})$.</p>
<p>Das in Teil 3 dargestellte <strong>dezentrale Lernen</strong> verzichtet auf einen zentralen Server und ersetzt ihn durch eine Peer-to-Peer-Kommunikation zwischen Geräten.
Es gibt kein globales Modell mehr wie beim föderierten Lernen, da jedes Gerät sein eigenes, einzigartiges Modell hat, aber der Prozess kann so gestaltet werden, dass alle Modelle konvergieren.
Die wichtigsten Optimierungsalgorithmen für dezentralisiertes Lernen sind Gossip-Algorithmen (Konsensalgorithmen), die für Berechnungen und den Informationsaustausch in einem beliebig verbundenen Netzwerk von Knoten konzipiert sind.</p>
<h3 id="topologie">Topologie</h3>
<p>Beim dezentralisierten Lernen hat sich gezeigt, dass die Konvergenzrate typischerweise von der Netztopologie abhängt.
Die Netzwerktopologie kann als Graph $G = (V, E)$ modelliert werden, wobei $\für alle v \ in V$ ein Gerät und $\für alle {i, j} \ in E$ einen Kommunikationskanal zwischen zwei Geräten $i$ und $j$ darstellt.
Insbesondere die spektrale Lücke $\delta \geq 0$ der normalisierten Adjazenzmatrix ($W$) des Kommunikationstopologiegraphen ist wichtig für die Konvergenz.
<div class="details admonition info">
        <div class="details-summary admonition-title">
            <i class="icon fas fa-info-circle fa-fw"></i>Definition:  Spektrale Lücke<i class="details-icon fas fa-angle-right fa-fw"></i>
        </div>
        <div class="details-content">
            <div class="admonition-content">Angenommen $W \in [0, 1]^{n \times n}$ ist eine symmetrische doppelt stochastische Matrix mit Eigenwerten $1 = |\lambda_1(W)| &gt; |\lambda_2(W)| \geq &hellip; \geq |\lambda_n(W)|$. Dann ist die spektrale Lücke:
$$ \delta = 1 - |\lambda_2(W)| \in (0, 1] $$</div>
        </div>
    </div></p>
<p>Der Grund dafür liegt in der Beziehung zwischen den spektralen Lücken und der Konnektivität des Graphen.
Zum Beispiel hat ein vollständig verbundener Graph ein $\delta = O(1)$, während ein Ringgraph mit Knotengrad 2 $\delta = O(1/n^2)$ auf $n$ Knoten hat.</p>
<div class="details admonition tip open">
        <div class="details-summary admonition-title">
            <i class="icon fas fa-lightbulb fa-fw"></i>Spektrallücken für unsere Graphen<i class="details-icon fas fa-angle-right fa-fw"></i>
        </div>
        <div class="details-content">
            <div class="admonition-content"><p>Spektrale Lücken für die normalisierte, doppelt stochastische Adjazenzmatrix für unsere Graphen mit $4$, $16$ und $32$ Knoten.</p>
<table>
<thead>
<tr>
<th>Knoten</th>
<th>4</th>
<th>16</th>
<th>32</th>
</tr>
</thead>
<tbody>
<tr>
<td>Vollständig verbundener Graph</td>
<td>.666</td>
<td>.933</td>
<td>.968</td>
</tr>
<tr>
<td>Zyklusgraph</td>
<td>1.0</td>
<td>.076</td>
<td>.019</td>
</tr>
<tr>
<td>Ring von Cliquen</td>
<td>1.0</td>
<td>.104</td>
<td>.028</td>
</tr>
<tr>
<td>Torus 2D</td>
<td>1.0</td>
<td>.195</td>
<td>.051</td>
</tr>
</tbody>
</table>
</div>
        </div>
    </div>
<h3 id="differentielle-privatsphäre">Differentielle Privatsphäre</h3>
<p><strong>Differential Privacy (DP)</strong> ist eine Methode zur Veröffentlichung bestimmter Daten in einer die Privatsphäre wahrenden Weise. Die Tatsache, dass man bei DP den Umfang der preisgegebenen Privatsphäre quantitativ kontrollieren kann, unterscheidet es von dem Konzept der Anonymisierung oder der Erzeugung synthetischer Daten.</p>
<div class="details admonition info">
        <div class="details-summary admonition-title">
            <i class="icon fas fa-info-circle fa-fw"></i>Definition: $(\epsilon, \delta)$ - differential privacy<i class="details-icon fas fa-angle-right fa-fw"></i>
        </div>
        <div class="details-content">
            <div class="admonition-content">Stellen wir uns den DP-Algorithmus als eine Funktion $\mathcal{A}$ und unseren Originaldatensatz als $D$ vor, so ist $\mathcal{A}(D)$ das Ergebnis der Anwendung des DP-Mechanismus auf die Originaldaten. Der Algorithmus $\mathcal{A}$ bietet $(\epsilon, \delta)$-differentielle Privatsphäre, wenn für alle Datensätze $D_{1}$ und $D_2$, die sich in einem einzigen Element unterscheiden (d.h. die Daten einer Person), und alle Teilmengen $S$ von $\textrm{image}(\mathcal{A})$:
$$
\mathbb{P}[\mathcal{A}(D_1) \in S] \leq e^{\epsilon} \mathbb{P}[\mathcal{A}(D_2) \in S] + \delta,
$$
wobei die Wahrscheinlichkeit in Bezug auf die vom Mechanismus $\mathcal{A}$ verwendete Zufälligkeit genommen wird. Ein häufiges Dilemma bei Technologien zur Wahrung der Privatsphäre ist, dass je niedriger wir $\epsilon$ (das Budget für die Privatsphäre) setzen, desto &ldquo;zufälliger&rdquo; muss $\mathcal{A}$ sein, was zu Daten führt, die einen geringeren Nutzen besitzen, aber gleichzeitig weniger Informationen preisgeben.</div>
        </div>
    </div>
<p>Um auf das vorliegende Projekt zurückzukommen: Für das Training von Modellen des maschinellen Lernens werden oft große, repräsentative Datensätze benötigt, die möglicherweise aus der Bevölkerung stammen und sensible Informationen enthalten. Die Modelle sollten keine privaten Informationen in diesen Datensätzen preisgeben, weshalb wir die <code>opacus</code>- Bibliothek in Python für das Training der Modelle in einer datenschutzfreundlichen Weise verwendet haben. Opacus ändert das Standard-Lernverfahren, indem es drei zusätzliche Schritte in den Backpropagation-Schritt einfügt. Anstatt einfach die Gradienten zu berechnen und sie zur Aktualisierung der entsprechenden Parameter zu verwenden, basiert <code>opacus</code> auf dem DP-SGD-Algorithmus.</p>
<h3 id="verwandte-arbeiten">Verwandte Arbeiten</h3>
<p>In <a href="http://arxiv.org/abs/2002.12688" target="_blank" rel="noopener noreffer">Neglia et. al.</a> untersuchen die Autoren den Widerspruch zwischen der Theorie, die besagt, dass die Topologie der Arbeiterkommunikation einen starken Einfluss auf die Anzahl der zur Konvergenz benötigten Epochen haben sollte, und Experimenten, die das Gegenteil zeigen.</p>
<div class="details admonition tip open">
        <div class="details-summary admonition-title">
            <i class="icon fas fa-lightbulb fa-fw"></i>Neglia et. al. on network topology<i class="details-icon fas fa-angle-right fa-fw"></i>
        </div>
        <div class="details-content">
            <div class="admonition-content">Die Autoren merken <a href="http://arxiv.org/abs/1005.2012%20http://dx.doi.org/10.1109/TAC.2011.2161027" target="_blank" rel="noopener noreffer">hier</a> und <a href="https://asu.mit.edu/sites/default/files/documents/publications/distributed-journal-final.pdf" target="_blank" rel="noopener noreffer">hier</a> an, dass sie die Anzahl der Iterationen $T$ zur Approximation der minimalen Zielfunktion durch den gewünschten Fehler $\epsilon$ berechnen als
$$
T \in O(\frac{1}{\epsilon^2 \delta})
$$
und kommen zu dem Schluss, dass eine stärker vernetzte Topologie zu einer schnelleren Konvergenz führt.
Ihre Experimente zeigen jedoch, dass bei statistisch ähnlichen Datensätzen die Topologie nur einen geringen Einfluss auf die Anzahl der zur Konvergenz benötigten Epochen hat.</div>
        </div>
    </div>
<h2 id="experimente">Experimente</h2>
<p>Um die Leistung der verschiedenen Architekturen zu testen, betrachten wir die Aufgabe, die Klasse einer Ziffer aus dem <code>MNIST</code>-Datensatz vorherzusagen. Der Trainings- und Testdatensatz besteht aus $60000$ bzw. $10000$ Stichproben, und wir haben die Eingabedaten normalisiert. Die Daten werden zwischen den Knoten entweder <em>zufällig</em> oder <em>gleichmäßig</em> aufgeteilt, d.h. jeder Knoten erhält eine zufällige Menge an Daten oder genau die gleiche Menge. Jeder Knoten erhält eine eindeutige Partition der Daten. Außerdem gibt es eine <em>iid</em>- oder <em>non-iid</em>-Einstellung, bei der die Klassen zufällig auf die einzelnen Knoten verteilt werden, während bei der letzteren Einstellung jeder Knoten nur Daten aus bestimmten Klassen erhält.</p>
<h3 id="versuchsaufbau">Versuchsaufbau</h3>
<p>Für unsere Experimente vergleichen wir vier verschiedene Topologien: den vollständig verbundenen, den Ring-, den Cliquen- und den Torus-Graphen, wobei wir die Anzahl der Knoten zwischen $[4,\ 16,\ 32]$ variieren lassen. Wir haben die Tests für alle Datenkombinationen von [iid, non-iid] und [uniform, random] durchgeführt und außerdem einmal in einer nicht-privaten und einmal in einer privaten Umgebung mit den Parametern $\epsilon = 1.1$ und $\delta = 1e-6$.
Hinsichtlich der Lernrate haben wir für jede Einstellung $\gamma \in [0.0001,\ 0.001,\ 0.01,\ 0.1, \ 0.2]$ ausprobiert und uns für diejenige entschieden, die zur besten Konvergenz führte.</p>
<p>In jeder Epoche wird jede Stichprobe genau einmal von einem Knoten zum Training verwendet. In der Einstellung <em>iid uniform</em> mit $[4,\ 16,\ 32]$ Knoten erhält jeder Knoten $[15000,\ 3750,\ 1875]$ Stichproben, in allen anderen Einstellungen unterscheiden sich die Knoten in ihrer verfügbaren Menge an Stichproben. Wenn ein Knoten weniger Stichproben als seine Nachbarn hat und alle seine Stapel in einer Epoche verwendet hat, hört er auf, seine Gewichte zu teilen, aktualisiert aber weiterhin sein Modell mit dem Gewicht, das die anderen Knoten mit ihm teilen, bis die Epoche abgeschlossen ist.</p>
<h2 id="ergebnisse">Ergebnisse</h2>
<p>In unserem <em>iid uniform</em> Setup erwarteten wir ähnliche Ergebnisse wie in <a href="http://arxiv.org/abs/1706.07880" target="_blank" rel="noopener noreffer">Jiang2017</a>, wo die Autoren feststellten, dass die Konvergenz der durchschnittlichen Genauigkeit bei spärlicheren Netzwerken tendenziell schneller erfolgt.
Unsere Ergebnisse bestätigen diese Ergebnisse, und wir sehen, dass unabhängig von der Topologie der durchschnittliche Knoten am Ende zu einer ähnlichen Genauigkeit konvergiert:
<figure><a class="lightgallery" href="/images/topology_testrun_iid_uniform_data-1.png" title="topology_testrun_iid_uniform_data.pdf" data-thumbnail="/images/topology_testrun_iid_uniform_data-1.png" data-sub-html="<h2>Convergence plots from the iid uniform and no differential privacy setup.</h2><p>topology_testrun_iid_uniform_data.pdf</p>">
        <img
            class="lazyload"
            src="/svg/loading.min.svg"
            data-src="/images/topology_testrun_iid_uniform_data-1.png"
            data-srcset="/images/topology_testrun_iid_uniform_data-1.png, /images/topology_testrun_iid_uniform_data-1.png 1.5x, /images/topology_testrun_iid_uniform_data-1.png 2x"
            data-sizes="auto"
            alt="/images/topology_testrun_iid_uniform_data-1.png" />
    </a><figcaption class="image-caption">Convergence plots from the iid uniform and no differential privacy setup.</figcaption>
    </figure></p>
<p>Jiang et al. stellen dann fest, dass spärliche Topologien zwar schneller konvergieren, dass aber dichte Topologien stabiler sind und daher für ein dezentrales Lernparadigma entscheidend sind.
Durch die Einführung von zwei neuen Paradigmen, einer <em>non-iid</em> Dateneinstellung und einer datenschutzbewussten Einstellung, hinterfragen wir diese Behauptung.
In der <em>non-iid</em> Datenumgebung geben wir jedem Knoten nur $3$ der $10$ Klassen von MNIST und erwarten, dass spärlichere Topologien mit dem Lernen zu kämpfen haben, während dichtere Topologien, die mehr Gewichte mitteln, in der Lage sein sollten, die Informationen der anderen Klassen schneller zu übernehmen.
Wir haben erwartet, dass die Zyklusgraphen mit dem Knotengrad $2$ am schlechtesten lernen, da sie bei $3$ Knoten nur maximal $9$ der $10$ Klassenbezeichnungen abdecken.</p>
<p><figure><a class="lightgallery" href="/images/topology_testrun_non_iid_uniform-1.png" title="topology_testrun_non_iid_uniform.pdf" data-thumbnail="/images/topology_testrun_non_iid_uniform-1.png" data-sub-html="<h2>Convergence plots from the non-iid uniform and no differential privacy setup.</h2><p>topology_testrun_non_iid_uniform.pdf</p>">
        <img
            class="lazyload"
            src="/svg/loading.min.svg"
            data-src="/images/topology_testrun_non_iid_uniform-1.png"
            data-srcset="/images/topology_testrun_non_iid_uniform-1.png, /images/topology_testrun_non_iid_uniform-1.png 1.5x, /images/topology_testrun_non_iid_uniform-1.png 2x"
            data-sizes="auto"
            alt="/images/topology_testrun_non_iid_uniform-1.png" />
    </a><figcaption class="image-caption">Convergence plots from the non-iid uniform and no differential privacy setup.</figcaption>
    </figure></p>
<p>Wie in der obigen Abbildung für die Einstellung <em>non-iid</em> zu sehen ist, bleibt die Testkurve des vollständig verbundenen Graphen mit $16$ und $32$ Knoten hinter den anderen spärlicheren Graphen zurück.
Der Zyklusgraph schneidet im Allgemeinen gut ab und ist sogar die leistungsstärkste Topologie im Szenario mit $16$ Knoten.
Wir vermuten, dass unser <em>nicht-ideales</em> Szenario möglicherweise nicht schwierig genug war, was bedeutet, dass auch in <em>nicht-idealen</em> Szenarien Sparsamkeit erwünscht sein kann, wenn die Daten bis zu einem gewissen Grad gemischt werden.
Für weitere Experimente besteht die Möglichkeit, die <em>non-iid</em> Einstellung viel schwieriger zu machen, indem man entweder die Anzahl der Klassen einschränkt oder eine schwierigere Aufgabe wie <code>CIFAR-10</code> versucht.</p>
<p>Bei der Einstellung &ldquo;Privatsphäre&rdquo; war unsere Erwartung, dass alle Modelle im Allgemeinen langsamer konvergieren und wegen der Einführung von Rauschen nicht die gleiche Testgenauigkeit erreichen würden. Genau das ist eingetreten, und der Unterschied zwischen dem nicht-iiden einheitlichen <em>privaten</em> und <em>nicht-privaten</em> Setup ist deutlich zu erkennen:</p>
<p><figure><a class="lightgallery" href="/images/topology_testrun_non_iid_uniform_private-1.png" title="topology_testrun_non_iid_uniform_private.pdf" data-thumbnail="/images/topology_testrun_non_iid_uniform_private-1.png" data-sub-html="<h2>Convergence plots from the non-iid uniform and differential privacy setup.</h2><p>topology_testrun_non_iid_uniform_private.pdf</p>">
        <img
            class="lazyload"
            src="/svg/loading.min.svg"
            data-src="/images/topology_testrun_non_iid_uniform_private-1.png"
            data-srcset="/images/topology_testrun_non_iid_uniform_private-1.png, /images/topology_testrun_non_iid_uniform_private-1.png 1.5x, /images/topology_testrun_non_iid_uniform_private-1.png 2x"
            data-sizes="auto"
            alt="/images/topology_testrun_non_iid_uniform_private-1.png" />
    </a><figcaption class="image-caption">Convergence plots from the non-iid uniform and differential privacy setup.</figcaption>
    </figure></p>
<p>Die Einführung von Rauschen scheint das Lernen in der schwierigeren Einstellung <em>non-iid random</em> und bei Erhöhung der Anzahl der Knoten stärker zu behindern. In Bezug auf weitere Experimente wäre es lohnenswert, die Leistung des Netzwerks über mehrere Werte des Privatsphärenbudgets $\epsilon$ zu untersuchen, wie es in [dieser Arbeit] (<a href="https://arxiv.org/pdf/1905.12101.pdf" target="_blank" rel="noopener noreffer">https://arxiv.org/pdf/1905.12101.pdf</a>) getan wurde.</p>
<h2 id="schlussfolgerung">Schlussfolgerung</h2>
<p>Zusammenfassend lässt sich sagen, dass wir einen Rahmen geschaffen haben, der es ermöglicht, verschiedene Szenarien in einem dezentralen maschinellen Lernumfeld zu simulieren. Die wichtigsten Faktoren, die wir angepasst haben, waren die Art der Topologie, die Anzahl der Knoten, die Annahme der Unabhängigkeit und identischen Verteilung der Daten und das Hinzufügen von differentieller Privatsphäre. Unsere Experimente zeigen, dass spärliche Graphen, gemessen an der spektralen Lücke der Topologie, selbst in schwierigen Situationen unerwartet gut abschneiden, während dichter verbundene Graphen mit zunehmender Knotenzahl langsamer lernen. Dies widerspricht der bisherigen Theorie, steht aber im Einklang mit anderen experimentellen Ergebnissen.</p></div><div class="post-footer" id="post-footer">
    <div class="post-info">
        <div class="post-info-line">
            <div class="post-info-mod">
                <span>Aktualisiert am 2021-08-28</span>
            </div>
            <div class="post-info-license"></div>
        </div>
        <div class="post-info-line">
            <div class="post-info-md"></div>
            <div class="post-info-share">
                <span></span>
            </div>
        </div>
    </div>

    <div class="post-info-more">
        <section class="post-tags"><i class="fas fa-tags fa-fw"></i>&nbsp;<a href="/de/tags/decentralized/">decentralized</a>,&nbsp;<a href="/de/tags/machinelearning/">machinelearning</a>,&nbsp;<a href="/de/tags/privacy/">privacy</a></section>
        <section>
            <span><a href="javascript:void(0);" onclick="window.history.back();">Zurück</a></span>&nbsp;|&nbsp;<span><a href="/de/">Startseite</a></span>
        </section>
    </div>

    <div class="post-nav"></div>
</div>
</article></div>
            </main><footer class="footer">
        <div class="footer-container"><div class="footer-line"><i class="far fa-copyright fa-fw"></i><span itemprop="copyrightYear">2021</span><span class="author" itemprop="copyrightHolder">&nbsp;<a href="/de/" target="_blank"></a></span></div>
        </div>
    </footer></div>

        <div id="fixed-buttons"><a href="#" id="back-to-top" class="fixed-button" title="nach oben">
                <i class="fas fa-arrow-up fa-fw"></i>
            </a><a href="#" id="view-comments" class="fixed-button" title="Kommentare anzeigen">
                <i class="fas fa-comment fa-fw"></i>
            </a>
        </div><link rel="stylesheet" href="/lib/lightgallery/lightgallery.min.css"><link rel="stylesheet" href="/lib/katex/katex.min.css"><link rel="stylesheet" href="/lib/cookieconsent/cookieconsent.min.css"><script type="text/javascript" src="/lib/smooth-scroll/smooth-scroll.min.js"></script><script type="text/javascript" src="/lib/autocomplete/autocomplete.min.js"></script><script type="text/javascript" src="/lib/algoliasearch/algoliasearch-lite.umd.min.js"></script><script type="text/javascript" src="/lib/lazysizes/lazysizes.min.js"></script><script type="text/javascript" src="/lib/lightgallery/lightgallery.min.js"></script><script type="text/javascript" src="/lib/lightgallery/lg-thumbnail.min.js"></script><script type="text/javascript" src="/lib/lightgallery/lg-zoom.min.js"></script><script type="text/javascript" src="/lib/clipboard/clipboard.min.js"></script><script type="text/javascript" src="/lib/katex/katex.min.js"></script><script type="text/javascript" src="/lib/katex/auto-render.min.js"></script><script type="text/javascript" src="/lib/cookieconsent/cookieconsent.min.js"></script><script type="text/javascript">window.config={"code":{"copyTitle":"In Zwischenablage kopieren","maxShownLines":10},"comment":{},"cookieconsent":{"content":{"dismiss":"Zustimmen","link":"Erfahren Sie mehr","message":"Diese Website verwendet Cookies, um Ihre Erfahrung zu verbessern."},"enable":true,"palette":{"button":{"background":"#f0f0f0"},"popup":{"background":"#1aa3ff"}},"theme":"edgeless"},"lightGallery":{"actualSize":false,"exThumbImage":"data-thumbnail","hideBarsDelay":2000,"selector":".lightgallery","speed":400,"thumbContHeight":80,"thumbWidth":80,"thumbnail":true},"math":{"delimiters":[{"display":true,"left":"$$","right":"$$"},{"display":true,"left":"\\[","right":"\\]"},{"display":false,"left":"$","right":"$"},{"display":false,"left":"\\(","right":"\\)"}],"strict":false},"search":{"algoliaAppID":"PASDMWALPK","algoliaIndex":"index.de","algoliaSearchKey":"b42948e51daaa93df92381c8e2ac0f93","highlightTag":"em","maxResultLength":10,"noResultsFound":"Keine Ergebnisse gefunden","snippetLength":50,"type":"algolia"}};</script><script type="text/javascript" src="/js/theme.min.js"></script></body>
</html>
