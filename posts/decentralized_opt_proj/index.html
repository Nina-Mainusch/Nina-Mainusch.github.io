<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="robots" content="noodp" />
        <meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1">
        <title>Decentralized ML: Experiments on Topology and Privacy - Nina Mainusch</title><meta name="Description" content="Decentralized machine learning is an important type of machine learning used to address problems such as data locality, ownership, privacy and high communication costs. We contribute an empirical evaluation of the impact of topology and differential privacy on model convergence and error in a decentralized learning system."><meta property="og:title" content="Decentralized ML: Experiments on Topology and Privacy" />
<meta property="og:description" content="Decentralized machine learning is an important type of machine learning used to address problems such as data locality, ownership, privacy and high communication costs. We contribute an empirical evaluation of the impact of topology and differential privacy on model convergence and error in a decentralized learning system." />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://example.org/posts/decentralized_opt_proj/" /><meta property="og:image" content="http://example.org/logo.png"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-08-28T12:57:25+02:00" />
<meta property="article:modified_time" content="2021-08-28T12:57:25+02:00" />

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="http://example.org/logo.png"/>

<meta name="twitter:title" content="Decentralized ML: Experiments on Topology and Privacy"/>
<meta name="twitter:description" content="Decentralized machine learning is an important type of machine learning used to address problems such as data locality, ownership, privacy and high communication costs. We contribute an empirical evaluation of the impact of topology and differential privacy on model convergence and error in a decentralized learning system."/>
<meta name="application-name" content="LoveIt">
<meta name="apple-mobile-web-app-title" content="LoveIt"><meta name="theme-color" content="#ffffff"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
        <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="http://example.org/posts/decentralized_opt_proj/" /><link rel="stylesheet" href="/lib/normalize/normalize.min.css"><link rel="stylesheet" href="/css/style.min.css"><link rel="stylesheet" href="/lib/fontawesome-free/all.min.css"><link rel="stylesheet" href="/lib/animate/animate.min.css"><script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "headline": "Decentralized ML: Experiments on Topology and Privacy",
        "inLanguage": "en",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "http:\/\/example.org\/posts\/decentralized_opt_proj\/"
        },"genre": "posts","keywords": "decentralized, machinelearning, privacy","wordcount":  2012 ,
        "url": "http:\/\/example.org\/posts\/decentralized_opt_proj\/","datePublished": "2021-08-28T12:57:25+02:00","dateModified": "2021-08-28T12:57:25+02:00","license": "This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.","publisher": {
            "@type": "Organization",
            "name": "Nina"},"author": {
                "@type": "Person",
                "name": "Nina"
            },"description": "Decentralized machine learning is an important type of machine learning used to address problems such as data locality, ownership, privacy and high communication costs. We contribute an empirical evaluation of the impact of topology and differential privacy on model convergence and error in a decentralized learning system."
    }
    </script></head>
    <body header-desktop="fixed" header-mobile="auto"><script type="text/javascript">(window.localStorage && localStorage.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('theme', 'dark');</script>

        <div id="mask"></div><div class="wrapper"><header class="desktop" id="header-desktop">
    <div class="header-wrapper">
        <div class="header-title">
            <a href="/" title="Nina Mainusch">Nina Mogan Mainusch</a>
        </div>
        <div class="menu">
            <div class="menu-inner"><a class="menu-item" href="/about/"> About </a><a class="menu-item" href="/posts/"> Posts </a><a class="menu-item" href="https://github.com/nina-mainusch" title="GitHub" rel="noopener noreffer" target="_blank"><i class='fab fa-github fa-fw'></i>  </a><a class="menu-item" href="/documentation/" title="legal information"><i class='fa fa-gavel fa-fw'></i>  </a><span class="menu-item delimiter"></span><a href="javascript:void(0);" class="menu-item language" title="Select Language">English<i class="fas fa-chevron-right fa-fw"></i>
                        <select class="language-select" id="language-select-desktop" onchange="location = this.value;"><option value="/posts/decentralized_opt_proj/" selected>English</option><option value="/de/posts/decentralized_opt_proj/">Deutsch</option><option value="/fr/posts/decentralized_opt_proj/">Français</option></select>
                    </a><span class="menu-item search" id="search-desktop">
                        <input type="text" placeholder="Search titles or contents..." id="search-input-desktop">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-desktop" title="Search">
                            <i class="fas fa-search fa-fw"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-desktop" title="Clear">
                            <i class="fas fa-times-circle fa-fw"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-desktop">
                            <i class="fas fa-spinner fa-fw fa-spin"></i>
                        </span>
                    </span><a href="javascript:void(0);" class="menu-item theme-switch" title="Switch Theme">
                    <i class="fas fa-adjust fa-fw"></i>
                </a>
            </div>
        </div>
    </div>
</header><header class="mobile" id="header-mobile">
    <div class="header-container">
        <div class="header-wrapper">
            <div class="header-title">
                <a href="/" title="Nina Mainusch">Nina Mogan Mainusch</a>
            </div>
            <div class="menu-toggle" id="menu-toggle-mobile">
                <span></span><span></span><span></span>
            </div>
        </div>
        <div class="menu" id="menu-mobile"><div class="search-wrapper">
                    <div class="search mobile" id="search-mobile">
                        <input type="text" placeholder="Search titles or contents..." id="search-input-mobile">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-mobile" title="Search">
                            <i class="fas fa-search fa-fw"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-mobile" title="Clear">
                            <i class="fas fa-times-circle fa-fw"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-mobile">
                            <i class="fas fa-spinner fa-fw fa-spin"></i>
                        </span>
                    </div>
                    <a href="javascript:void(0);" class="search-cancel" id="search-cancel-mobile">
                        Cancel
                    </a>
                </div><a class="menu-item" href="/about/" title="">About</a><a class="menu-item" href="/posts/" title="">Posts</a><a class="menu-item" href="https://github.com/nina-mainusch" title="GitHub" rel="noopener noreffer" target="_blank"><i class='fab fa-github fa-fw'></i></a><a class="menu-item" href="/documentation/" title="legal information"><i class='fa fa-gavel fa-fw'></i></a><a href="javascript:void(0);" class="menu-item theme-switch" title="Switch Theme">
                <i class="fas fa-adjust fa-fw"></i>
            </a><a href="javascript:void(0);" class="menu-item" title="Select Language">English<i class="fas fa-chevron-right fa-fw"></i>
                    <select class="language-select" onchange="location = this.value;"><option value="/posts/decentralized_opt_proj/" selected>English</option><option value="/de/posts/decentralized_opt_proj/">Deutsch</option><option value="/fr/posts/decentralized_opt_proj/">Français</option></select>
                </a></div>
    </div>
</header>
<div class="search-dropdown desktop">
    <div id="search-dropdown-desktop"></div>
</div>
<div class="search-dropdown mobile">
    <div id="search-dropdown-mobile"></div>
</div>
<main class="main">
                <div class="container"><div class="toc" id="toc-auto">
            <h2 class="toc-title">Contents</h2>
            <div class="toc-content" id="toc-content-auto"></div>
        </div><article class="page single"><h1 class="single-title animated flipInX">Decentralized ML: Experiments on Topology and Privacy</h1><div class="post-meta">
            <div class="post-meta-line"><span class="post-author"><a href="/" title="Author" rel=" author" class="author"><i class="fas fa-user-circle fa-fw"></i>Nina</a></span>&nbsp;<span class="post-category">included in <a href="/categories/decentralized-ml/"><i class="far fa-folder fa-fw"></i>Decentralized ML</a></span></div>
            <div class="post-meta-line"><i class="far fa-calendar-alt fa-fw"></i>&nbsp;<time datetime="2021-08-28">2021-08-28</time>&nbsp;<i class="fas fa-pencil-alt fa-fw"></i>&nbsp;2012 words&nbsp;
                <i class="far fa-clock fa-fw"></i>&nbsp;10 minutes&nbsp;</div>
        </div><div class="details toc" id="toc-static"  kept="">
                <div class="details-summary toc-title">
                    <span>Contents</span>
                    <span><i class="details-icon fas fa-angle-right"></i></span>
                </div>
                <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li><a href="#abstract">Abstract</a></li>
    <li><a href="#introduction">Introduction</a>
      <ul>
        <li><a href="#what-is-our-motivation">What is our motivation?</a></li>
        <li><a href="#what-is-decentralized-machine-learning-about">What is decentralized machine learning about?</a></li>
        <li><a href="#topology">Topology</a></li>
        <li><a href="#differential-privacy">Differential Privacy</a></li>
        <li><a href="#related-work">Related Work</a></li>
      </ul>
    </li>
    <li><a href="#experiments">Experiments</a>
      <ul>
        <li><a href="#experimental-setup">Experimental setup</a></li>
      </ul>
    </li>
    <li><a href="#results">Results</a></li>
    <li><a href="#conclusion">Conclusion</a></li>
  </ul>
</nav></div>
            </div><div class="content" id="content"><h2 id="abstract">Abstract</h2>
<p>Decentralized machine learning is an important type of machine learning used to address problems such as data locality, ownership, privacy and high communication costs.
In our project for the class <a href="https://edu.epfl.ch/coursebook/en/optimization-for-machine-learning-CS-439" target="_blank" rel="noopener noreffer">CS-439 - Optimization for Machine Learning</a> at EPFL two colleagues and I contributed an empirical evaluation of the impact of topology and differential privacy on model convergence and error in a decentralized learning system. Check out our <a href="https://github.com/Devrim-Celik/OptimML" target="_blank" rel="noopener noreffer">github repository</a> or read our <a href="/documents_/OptML_Project_Report.pdf" rel="">research paper</a>. No worries, if not I will work you through what we have done in this post.</p>
<div class="details admonition tip open">
        <div class="details-summary admonition-title">
            <i class="icon fas fa-lightbulb fa-fw"></i>Main finding<i class="details-icon fas fa-angle-right fa-fw"></i>
        </div>
        <div class="details-content">
            <div class="admonition-content">The main finding was that sparse graphs, as measured by the spectral gap of the topology, perform unexpectedly well even in difficult situations, while more densely connected graphs learn more slowly as the number of nodes increases.</div>
        </div>
    </div>
<h2 id="introduction">Introduction</h2>
<h3 id="what-is-our-motivation">What is our motivation?</h3>
<p>Centralized forms of machine learning have been thoroughly explored and are the current state of the art but have significant problems dealing with issues such as data locality, ownership, privacy and high communication costs.
Decentralized machine learning addresses these problems by transferring control from a primary server to each individual edge device.
This allows data to reside at the source, protecting privacy, and lowering communication costs by shifting the medium from the data itself to a compressed representation.
While decentralized networks do enhance privacy there are still problems since sensitive data can be learned from something as simple as a shared gradient.
This motivates our exploration of adding differential privacy to the communication to ensure maximum privacy.</p>
<h3 id="what-is-decentralized-machine-learning-about">What is decentralized machine learning about?</h3>
<p><figure><a class="lightgallery" href="/images/centralized-1.png" title="Visually explained" data-thumbnail="/images/centralized-1.png" data-sub-html="<h2>Three types of organizational structures described graphically, where device units are depicted with rectangles, central units are depicted as clouds and arrows indicate communication between the units. Each brain represents a machine learning model that is trained by the corresponding unit. Each small colored box represents data residing on a device, and the colored rectangles represent a transformed representation of the data.</h2><p>Visually explained</p>">
        <img
            class="lazyload"
            src="/svg/loading.min.svg"
            data-src="/images/centralized-1.png"
            data-srcset="/images/centralized-1.png, /images/centralized-1.png 1.5x, /images/centralized-1.png 2x"
            data-sizes="auto"
            alt="/images/centralized-1.png" />
    </a><figcaption class="image-caption">Three types of organizational structures described graphically, where device units are depicted with rectangles, central units are depicted as clouds and arrows indicate communication between the units. Each brain represents a machine learning model that is trained by the corresponding unit. Each small colored box represents data residing on a device, and the colored rectangles represent a transformed representation of the data.</figcaption>
    </figure></p>
<p>In a traditional <strong>centralized machine learning</strong> setup, as visualized in the above figure, part 1, data is moved from remote devices to a central system where models can be classically trained using stochastic gradient descent (SGD) which for $T$ iterations has runtime $O(1/\sqrt{T})$ if the target function is convex.</p>
<p>In <strong>federated learning</strong>, shown in the figure, part 2, each device collaboratively trains a global machine learning model and is organized by a central server.
Data is not shared, and the devices send updates to a server which aggregates these and in turn updates the shared models across the devices.
The most well known optimization algorithm is federated averaging with convergence $O(\frac{HM}{T} + \frac{\sigma}{\sqrt{TKM}})$.</p>
<p><strong>Decentralized learning</strong>, depicted in part 3, does away with a central server, and replaces it with peer to peer communication between devices.
There is no longer a global model such as in federated learning, as each device has its own unique model, however the process can be designed so that all the models converge.
The main optimization algorithms for decentralized learning are gossip type (consensus) algorithms, which are designed for computation and information exchange in an arbitrarily connected network of nodes.</p>
<h3 id="topology">Topology</h3>
<p>In decentralized learning, it has been shown that the convergence rate typically depends on the network topology.
The network topology can be modeled as a graph $G = (V, E)$ with $\forall v \in V$ representing a device and $\forall {i, j} \in E$ representing a communication channel between two devices $i$ and $j$.
In particular the spectral gap $\delta \geq 0$ of the normalized adjacency matrix ($W$) of the communication topology graph is important in convergence.
<div class="details admonition info">
        <div class="details-summary admonition-title">
            <i class="icon fas fa-info-circle fa-fw"></i>Definition:  Spectral Gap<i class="details-icon fas fa-angle-right fa-fw"></i>
        </div>
        <div class="details-content">
            <div class="admonition-content">Assume $W \in [0, 1]^{n \times n}$ is a symmetric doubly stochastic matrix with eigenvalues $1 = |\lambda_1(W)| &gt; |\lambda_2(W)| \geq &hellip; \geq |\lambda_n(W)|$.
Then the spectral gap is
$$ \delta = 1 - |\lambda_2(W)| \in (0, 1] $$</div>
        </div>
    </div></p>
<p>The reason for this is because of the spectral gaps relation to connectivity of the graph.
For example, a fully connected graph has a $\delta = O(1)$ where a ring graph with node degree 2 has $\delta = O(1/n^2)$ on $n$ nodes.</p>
<div class="details admonition tip open">
        <div class="details-summary admonition-title">
            <i class="icon fas fa-lightbulb fa-fw"></i>Spectral gaps for our graphs<i class="details-icon fas fa-angle-right fa-fw"></i>
        </div>
        <div class="details-content">
            <div class="admonition-content"><p>Spectral gaps for the normalized, doubly stochastic adjacency matrix for our graphs with $4$, $16$, and $32$ nodes.</p>
<table>
<thead>
<tr>
<th>Nodes</th>
<th>4</th>
<th>16</th>
<th>32</th>
</tr>
</thead>
<tbody>
<tr>
<td>Fully Connected Graph</td>
<td>.666</td>
<td>.933</td>
<td>.968</td>
</tr>
<tr>
<td>Cycle Graph</td>
<td>1.0</td>
<td>.076</td>
<td>.019</td>
</tr>
<tr>
<td>Ring of Cliques</td>
<td>1.0</td>
<td>.104</td>
<td>.028</td>
</tr>
<tr>
<td>Torus 2D</td>
<td>1.0</td>
<td>.195</td>
<td>.051</td>
</tr>
</tbody>
</table>
</div>
        </div>
    </div>
<h3 id="differential-privacy">Differential Privacy</h3>
<p><strong>Differential privacy (DP)</strong> is a method of publishing certain pieces of data in a privacy-preserving fashion. The fact that in DP the practitioners can quantitatively control the amount of privacy that will be leaked differentiates it from the concept of anonymization or the generation of synthetic data.</p>
<div class="details admonition info">
        <div class="details-summary admonition-title">
            <i class="icon fas fa-info-circle fa-fw"></i>Definition:  $(\epsilon, \delta)$ - differential privacy<i class="details-icon fas fa-angle-right fa-fw"></i>
        </div>
        <div class="details-content">
            <div class="admonition-content">Let us represent the DP algorithm as a function $\mathcal{A}$ and our original dataset as $D$, thus $\mathcal{A}(D)$ is the result of applying the DP mechanism to the original data. The algorithm $\mathcal{A}$ is said to provide $(\epsilon, \delta)$-differential privacy if, for all datasets $D_{1}$ and $D_2$ that differ on a single element (i.e., the data of one person), and all subsets $S$ of $\textrm{image}(\mathcal{A})$:
$$
\mathbb{P}[\mathcal{A}(D_1) \in S] \leq e^{\epsilon} \mathbb{P}[\mathcal{A}(D_2) \in S]  + \delta,
$$
where the probability is taken with respect to the randomness used by mechanism $\mathcal{A}$. A common dilemma in privacy-preserving technologies is that the lower we set $\epsilon$ (the privacy budget), the &ldquo;more random&rdquo; $\mathcal{A}$ has to be, resulting in data that possesses less utility, but at the same time leaking less information.</div>
        </div>
    </div>
<p>Coming back to the project at hand, training machine learning models often requires large, representative datasets, which may be crowdsourced and contain sensitive information. The models should not expose private information in these datasets, which is why we utilized the <code>opacus</code> library within python for training the models in a privacy-preserving fashion. Opacus alters the standard learning procedure by adding three extra steps in the backpropagation step. Instead of simply calculating the gradients and using them to update the corresponding parameters, <code>opacus</code> is based on DP-SGD algorithm.</p>
<h3 id="related-work">Related Work</h3>
<p>In <a href="http://arxiv.org/abs/2002.12688" target="_blank" rel="noopener noreffer">Neglia et. al.</a> the authors look at the contradiction between theory which states that worker communication topology should have a strong impact on the number of epochs needed to converge and experiments which show the opposite conclusion.</p>
<div class="details admonition tip open">
        <div class="details-summary admonition-title">
            <i class="icon fas fa-lightbulb fa-fw"></i>Neglia et. al. on network topology<i class="details-icon fas fa-angle-right fa-fw"></i>
        </div>
        <div class="details-content">
            <div class="admonition-content">The authors note <a href="http://arxiv.org/abs/1005.2012%20http://dx.doi.org/10.1109/TAC.2011.2161027" target="_blank" rel="noopener noreffer">here</a> and <a href="https://asu.mit.edu/sites/default/files/documents/publications/distributed-journal-final.pdf" target="_blank" rel="noopener noreffer">here</a> that they calculate the number of iterations $T$ to approximate the minimum objective function by the desired error $\epsilon$ as
$$
T \in O(\frac{1}{\epsilon^2 \delta})
$$
and conclude that a more connected network topology leads to faster convergence.
However, their experiments show that when a dataset is statistically similar, the topology has little influence on the number of epochs needed to converge.</div>
        </div>
    </div>
<h2 id="experiments">Experiments</h2>
<p>To test the performance of the different architectures, we consider the task of predicting the class of a digit in the <code>MNIST</code> dataset. The training and test set consist of $60000$ and $10000$ samples, respectively, and we normalized the input data. The data is partitioned between the nodes either <em>randomly</em> or <em>uniformly</em>, i.e. each node gets a random amount of data or exactly the same amount. Each node receives a unique partition of the data. Furthermore, we have an <em>iid</em> or <em>non-iid</em> setting, where in the former, the classes are distributed randomly between each node, but in the latter, every node only gets data from certain classes.</p>
<h3 id="experimental-setup">Experimental setup</h3>
<p>For our experiments we compare four different topologies: the fully connected, ring, circle of cliques and torus graph, where we let the number of nodes vary between $[4,\ 16,\ 32]$. We ran the tests for all data combinations of [iid, non-iid] and [uniform, random] and furthermore once in a non-private and once in a private setting with parameters $\epsilon = 1.1$ and $\delta = 1e-6$.
In regard to the learning rate, for each setup we tried $\gamma \in [0.0001,\ 0.001,\ 0.01,\ 0.1, \ 0.2]$ and settled for the one which led to the best convergence.</p>
<p>In every epoch, each sample is used for training exactly once by a node. In the <em>iid uniform</em> setting with $[4,\ 16,\ 32]$ nodes, each node gets $[15000,\ 3750,\ 1875]$ samples, respectively, in all other settings the nodes differ in their available amount of samples. When a node has fewer samples than its neighbors and it has used all its batches in an epoch, it stops sharing its weights but still updates its model with the weight that the other nodes share with it, until the epoch is completed.</p>
<h2 id="results">Results</h2>
<p>In our <em>iid uniform</em> setup, we expected to see similar results as in <a href="http://arxiv.org/abs/1706.07880" target="_blank" rel="noopener noreffer">Jiang2017</a>, where the authors noticed that convergence of average accuracy tends to happen faster for sparser networks.
Our results corroborate these results, and we see that no matter the topology the average node converges to a similar accuracy in the end:
<figure><a class="lightgallery" href="/images/topology_testrun_iid_uniform_data-1.png" title="topology_testrun_iid_uniform_data.pdf" data-thumbnail="/images/topology_testrun_iid_uniform_data-1.png" data-sub-html="<h2>Convergence plots from the iid uniform and no differential privacy setup.</h2><p>topology_testrun_iid_uniform_data.pdf</p>">
        <img
            class="lazyload"
            src="/svg/loading.min.svg"
            data-src="/images/topology_testrun_iid_uniform_data-1.png"
            data-srcset="/images/topology_testrun_iid_uniform_data-1.png, /images/topology_testrun_iid_uniform_data-1.png 1.5x, /images/topology_testrun_iid_uniform_data-1.png 2x"
            data-sizes="auto"
            alt="/images/topology_testrun_iid_uniform_data-1.png" />
    </a><figcaption class="image-caption">Convergence plots from the iid uniform and no differential privacy setup.</figcaption>
    </figure></p>
<p>Jiang et. al. then state that sparse topologies may be faster for convergence, but that dense topologies are more stable and therefore crucial for a decentralized learning paradigm.
By introducing two new paradigms, a <em>non-iid</em> data setting and a privacy-conscious setting, we interrogate this claim.
In the <em>non-iid</em> data setting, we give each node only $3$ out of the $10$ classes from MNIST and expect sparser topologies to struggle with learning, while denser topologies that are averaging more weights should be able to incorporate information of the other classes faster.
We expected the cycle graphs with node degree $2$ to learn the worst, as amongst $3$ nodes they only cover at most $9$ out of the $10$ class labels.</p>
<p><figure><a class="lightgallery" href="/images/topology_testrun_non_iid_uniform-1.png" title="topology_testrun_non_iid_uniform.pdf" data-thumbnail="/images/topology_testrun_non_iid_uniform-1.png" data-sub-html="<h2>Convergence plots from the non-iid uniform and no differential privacy setup.</h2><p>topology_testrun_non_iid_uniform.pdf</p>">
        <img
            class="lazyload"
            src="/svg/loading.min.svg"
            data-src="/images/topology_testrun_non_iid_uniform-1.png"
            data-srcset="/images/topology_testrun_non_iid_uniform-1.png, /images/topology_testrun_non_iid_uniform-1.png 1.5x, /images/topology_testrun_non_iid_uniform-1.png 2x"
            data-sizes="auto"
            alt="/images/topology_testrun_non_iid_uniform-1.png" />
    </a><figcaption class="image-caption">Convergence plots from the non-iid uniform and no differential privacy setup.</figcaption>
    </figure></p>
<p>As can be seen in the above figure in the <em>non-iid</em> setting, the test curve of the fully connected graph with $16$ and $32$ nodes lags behind the other sparser graphs.
The cycle graph does well in general, even being the top performing topology in the $16$ node scenario.
We postulate that potentially our <em>non-iid</em> scenario was not difficult enough, which means that even in <em>non-iid</em> scenarios, sparsity may be desired if the data is shuffled to some extent.
For further experiments, there is room to make the <em>non-iid</em> setting much more difficult by either restricting the number of classes or trying a more difficult task such as <code>CIFAR-10</code>.</p>
<p>In the privacy setting, our expectation was that generally all models would converge slower and not reach the same level of test accuracy because of the introduction of noise. This is exactly what happened and can clearly be seen in the difference between the non-iid uniform <em>private</em> and <em>non-private</em> setup:</p>
<p><figure><a class="lightgallery" href="/images/topology_testrun_non_iid_uniform_private-1.png" title="topology_testrun_non_iid_uniform_private.pdf" data-thumbnail="/images/topology_testrun_non_iid_uniform_private-1.png" data-sub-html="<h2>Convergence plots from the non-iid uniform and differential privacy setup.</h2><p>topology_testrun_non_iid_uniform_private.pdf</p>">
        <img
            class="lazyload"
            src="/svg/loading.min.svg"
            data-src="/images/topology_testrun_non_iid_uniform_private-1.png"
            data-srcset="/images/topology_testrun_non_iid_uniform_private-1.png, /images/topology_testrun_non_iid_uniform_private-1.png 1.5x, /images/topology_testrun_non_iid_uniform_private-1.png 2x"
            data-sizes="auto"
            alt="/images/topology_testrun_non_iid_uniform_private-1.png" />
    </a><figcaption class="image-caption">Convergence plots from the non-iid uniform and differential privacy setup.</figcaption>
    </figure></p>
<p>The introduction of noise seems to hinder learning more in the more difficult setting of <em>non-iid random</em> and when increasing the number of nodes. Regarding further experiments, it would be worth investigating the performance of the network over multiple values of the privacy budget $\epsilon$, as done in <a href="https://arxiv.org/pdf/1905.12101.pdf" target="_blank" rel="noopener noreffer">this paper</a>.</p>
<h2 id="conclusion">Conclusion</h2>
<p>To conclude, we built a framework that allows to simulate different scenarios in a decentralized machine learning setting. The main factors we adjusted were the type of topology, the number of nodes, the independence and identically distributed assumption of the data, and adding differential privacy. Our experiments show that sparse graphs, as measured by the spectral gap of the topology, perform unexpectedly well even in difficult settings, while more densely connected graphs learn more slowly as the number of nodes increases. This contradicts previous theory but falls in line with other experimental results.</p>
</div><div class="post-footer" id="post-footer">
    <div class="post-info">
        <div class="post-info-line">
            <div class="post-info-mod">
                <span>Updated on 2021-08-28</span>
            </div>
            <div class="post-info-license"></div>
        </div>
        <div class="post-info-line">
            <div class="post-info-md"></div>
            <div class="post-info-share">
                <span></span>
            </div>
        </div>
    </div>

    <div class="post-info-more">
        <section class="post-tags"><i class="fas fa-tags fa-fw"></i>&nbsp;<a href="/tags/decentralized/">decentralized</a>,&nbsp;<a href="/tags/machinelearning/">machinelearning</a>,&nbsp;<a href="/tags/privacy/">privacy</a></section>
        <section>
            <span><a href="javascript:void(0);" onclick="window.history.back();">Back</a></span>&nbsp;|&nbsp;<span><a href="/">Home</a></span>
        </section>
    </div>

    <div class="post-nav"></div>
</div>
</article></div>
            </main><footer class="footer">
        <div class="footer-container"><div class="footer-line">Powered by Nina</div><div class="footer-line"><i class="far fa-copyright fa-fw"></i><span itemprop="copyrightYear">2021</span><span class="author" itemprop="copyrightHolder">&nbsp;<a href="/" target="_blank"></a></span></div>
        </div>
    </footer></div>

        <div id="fixed-buttons"><a href="#" id="back-to-top" class="fixed-button" title="Back to Top">
                <i class="fas fa-arrow-up fa-fw"></i>
            </a><a href="#" id="view-comments" class="fixed-button" title="View Comments">
                <i class="fas fa-comment fa-fw"></i>
            </a>
        </div><link rel="stylesheet" href="/lib/katex/katex.min.css"><script type="text/javascript" src="/lib/smooth-scroll/smooth-scroll.min.js"></script><script type="text/javascript" src="/lib/autocomplete/autocomplete.min.js"></script><script type="text/javascript" src="/lib/algoliasearch/algoliasearch-lite.umd.min.js"></script><script type="text/javascript" src="/lib/lazysizes/lazysizes.min.js"></script><script type="text/javascript" src="/lib/clipboard/clipboard.min.js"></script><script type="text/javascript" src="/lib/katex/katex.min.js"></script><script type="text/javascript" src="/lib/katex/auto-render.min.js"></script><script type="text/javascript">window.config={"code":{"copyTitle":"Copy to clipboard","maxShownLines":10},"comment":{},"math":{"delimiters":[{"display":true,"left":"$$","right":"$$"},{"display":true,"left":"\\[","right":"\\]"},{"display":false,"left":"$","right":"$"},{"display":false,"left":"\\(","right":"\\)"}],"strict":false},"search":{"algoliaAppID":"PASDMWALPK","algoliaIndex":"index.en","algoliaSearchKey":"b42948e51daaa93df92381c8e2ac0f93","highlightTag":"em","maxResultLength":10,"noResultsFound":"No results found","snippetLength":30,"type":"algolia"}};</script><script type="text/javascript" src="/js/theme.min.js"></script></body>
</html>
